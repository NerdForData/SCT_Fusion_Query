"""
RAG Implementation - FAISS-based Retrieval for Binning Research Papers
------------------------------------------------------------------------

Purpose:
    Perform semantic search over binning research paper chunks using FAISS
    vector similarity. Retrieves most relevant chunks for a given query.

Workflow:
    1. Load pre-built FAISS index and document metadata
    2. Generate query embedding using GPT4IFX embedding model
    3. Search FAISS index for top-k most similar chunks
    4. Return ranked results with source and similarity scores

Retrieval Method:
    - Semantic similarity using cosine distance (via normalized vectors)
    - FAISS IndexFlatIP for exact Inner Product search
    - Returns top-k most relevant chunks with scores

Inputs:
    binning_faiss.index - Pre-built FAISS index from embed_chunks.py
    binning_docs.jsonl  - Document metadata aligned with index

Usage:
    from rag_retriever import retrieve_binning_chunks
    
    chunks = retrieve_binning_chunks("What is bin yield optimization?", top_k=3)
    for chunk in chunks:
        print(f"Source: {chunk['source']}, Score: {chunk['score']}")
        print(f"Text: {chunk['text']}")

Integration:
    Used by sct_fusion_query.py when binning questions are detected
    by intent_classifier.py to provide domain-specific context.
"""

import json
from pathlib import Path
import numpy as np
import faiss
import openai, httpx
import base64, requests, urllib3

# Disable SSL warnings for internal Infineon network
urllib3.disable_warnings()

# ---------------------------------------------------------
# CONFIGURATION
# ---------------------------------------------------------
# GPT4IFX API settings
BASE_URL = "https://gpt4ifx.icp.infineon.com"  # Infineon internal LLM endpoint
MODEL_EMB = "sfr-embedding-mistral"             # Embedding model (4096 dimensions)
CERT_PATH = Path(__file__).parent.parent / "ca-bundle.crt"  # SSL certificate

# Authentication credentials
USERNAME = "INFINEON\\Aishwarya"
PASSWORD = "Gauri@123456789"

# FAISS index and document paths (generated by embed_chunks.py)
BIN_INDEX = Path(__file__).parent.parent / "System_KG_Implementation" / "binning_faiss.index"  # Vector index
BIN_DOCS  = Path(__file__).parent.parent / "System_KG_Implementation" / "binning_docs.jsonl"   # Document metadata

# ---------------------------------------------------------
# AUTHENTICATION
# ---------------------------------------------------------

def get_token():
    """
    Authenticate with GPT4IFX and retrieve Bearer token.
    Uses Basic Auth to exchange credentials for session token.
    
    Returns:
        str: Bearer token for API authentication
    """
    # Encode credentials as Base64 for Basic Auth
    tok = base64.b64encode(f"{USERNAME}:{PASSWORD}".encode()).decode()
    
    # Request Bearer token from authentication endpoint
    r = requests.get(f"{BASE_URL}/auth/token",
                     headers={"Authorization": f"Basic {tok}"},
                     verify=False)
    return r.headers.get("x-forwarded-access-token")

# Initialize OpenAI client with authentication
client = openai.OpenAI(
    api_key=get_token(),
    base_url=BASE_URL,
    http_client=httpx.Client(verify=False),
)

# ---------------------------------------------------------
# LOAD FAISS INDEX AND DOCUMENTS
# ---------------------------------------------------------
# Load pre-built FAISS index (generated by embed_chunks.py)
index = faiss.read_index(str(BIN_INDEX))

# Load document metadata (aligned with index by position)
docs = [json.loads(x) for x in BIN_DOCS.read_text(encoding='utf-8').splitlines()]

# ---------------------------------------------------------
# QUERY EMBEDDING AND RETRIEVAL
# ---------------------------------------------------------

def embed_query(q):
    """
    Generate embedding vector for a search query.
    
    Normalizes the query vector for cosine similarity search.
    FAISS IndexFlatIP with normalized vectors computes cosine similarity
    via inner product (IP = cosine for unit vectors).
    
    Args:
        q (str): Natural language search query
    
    Returns:
        np.ndarray: Normalized embedding vector (1, 4096) for FAISS search
    """
    # Generate embedding using GPT4IFX embedding model
    r = client.embeddings.create(
        model=MODEL_EMB,
        input=[q],
        encoding_format="float"
    )
    
    # Convert to numpy array
    v = np.array(r.data[0].embedding, dtype="float32")
    
    # Normalize vector for cosine similarity (L2 normalization)
    faiss.normalize_L2(v.reshape(1, -1))
    
    return v.reshape(1, -1)

def retrieve_binning_chunks(query, top_k=3):
    """
    Retrieve most relevant research paper chunks for a given query.
    
    Uses semantic similarity search over pre-built FAISS index to find
    the top-k most relevant chunks from binning research papers.
    
    Args:
        query (str): Natural language search query
        top_k (int): Number of top results to return (default: 3)
    
    Returns:
        list[dict]: List of ranked chunks with structure:
            {
                "source": str,      # Source PDF filename
                "score": float,     # Similarity score (0-1, higher = more similar)
                "text": str         # Chunk text (truncated to 600 chars)
            }
    
    Example:
        >>> chunks = retrieve_binning_chunks("bin yield optimization", top_k=3)
        >>> print(chunks[0]['score'])  # e.g., 0.87
        >>> print(chunks[0]['source'])  # e.g., "yield_analysis.pdf"
    """
    # Generate normalized query embedding
    qv = embed_query(query)
    
    # Search FAISS index for top-k nearest neighbors
    # D = distances (similarity scores), I = indices of matching documents
    D, I = index.search(qv, top_k)

    chunks = []
    # Build result list with metadata and scores
    for idx, score in zip(I[0], D[0]):
        # Retrieve document metadata from aligned list
        d = docs[idx]
        
        # Truncate text to 600 characters for context window efficiency
        text = d["text"][:600]
        
        chunks.append({
            "source": d["source"],        # Source PDF file
            "score": float(score),         # Similarity score
            "text": text                   # Chunk content (truncated)
        })
    
    return chunks